# Log da Sessão: Experimentação com Modelos LLM Hugging Face

**Data:** 09 de agosto de 2025  
**Objetivo:** Desenvolver script de interação com modelos LLM para controle de comportamento  
**Hardware:** GPU NVIDIA com até 15GB VRAM  

## 1. Iniciativa e Requisitos

### Requisito Inicial
- Criar script Python para interação com modelo LLM do Hugging Face
- Modelo compatível com GPU NVIDIA até 15GB VRAM
- Funcionalidade de diferentes comportamentos

### Primeira Iteração
**Modelo escolhido:** `microsoft/DialoGPT-medium`
- **Justificativa:** Modelo leve, adequado para conversação
- **Problema identificado:** Respostas muito limitadas e de baixa qualidade

## 2. Evolução do Understanding sobre Configurações

### Descoberta: Arquivos de Configuração
**Pergunta:** "O modelo possui arquivos de configuração. O que são eles?"

**Aprendizado:**
- `config.json`: Padrão universal do Hugging Face contendo parâmetros de arquitetura
- `generation_config.json`: Configurações específicas para geração de texto
- **Portabilidade:** Mesma estrutura para todos os modelos (padrão AutoModel/AutoTokenizer)

### Limitações Identificadas
- Modificar `config.json` não altera comportamento fundamental do modelo
- Comportamento é controlado por **parâmetros de geração** e **engenharia de prompt**

## 3. Parâmetros de Controle de Comportamento

### Descoberta: Parâmetros de Geração
**Principais controles identificados:**

| Parâmetro | Função | Impacto no Comportamento |
|-----------|--------|-------------------------|
| `temperature` | Controla aleatoriedade | 0.1 = robótico, 1.0 = natural, 2.0 = criativo |
| `top_p` | Nucleus sampling | Controla diversidade de vocabulário |
| `top_k` | Limite de tokens candidatos | Foca em palavras mais prováveis |
| `repetition_penalty` | Penaliza repetições | Melhora qualidade das respostas |
| `max_new_tokens` | Limite de resposta | Controla verbosidade |

### Personalidades Implementadas
```python
"criativo": {
    "temperature": 0.9,
    "top_p": 0.95,
    "repetition_penalty": 1.0
},
"conservador": {
    "temperature": 0.3,
    "top_p": 0.8,
    "repetition_penalty": 1.2
},
"agente_tecnico": {
    "temperature": 0.2,
    "top_p": 0.7,
    "repetition_penalty": 1.25
}
```

## 4. Primeira Execução - Problemas Identificados

### Modelo: `microsoft/DialoGPT-medium`
**Problemas encontrados:**
1. **Attention mask warning:** Token de padding igual ao EOS token
2. **Respostas limitadas:** Apenas palavras soltas ("fim.", ".", "tamente.")
3. **Saída malformada:** Texto em português incorreto

**Diagnóstico:** Modelo muito simples para comportamentos complexos

## 5. Busca por Modelo Mais Robusto

### Tentativa com Modelo Especializado
**Modelo alvo:** `debisoft/mistral-7b-thinking-function_calling-logic-capturing-V0`
- **Motivação:** Especializado em reasoning e function calling
- **Problema:** Dependências ausentes (`protobuf`, `sentencepiece`)

### Resolução de Dependências
```bash
pip install protobuf sentencepiece
```

### Segundo Problema: Arquivos Ausentes
```
OSError: debisoft/mistral-7b-thinking-function_calling-logic-capturing-V0 does not appear to have a file named pytorch_model.bin, model.safetensors...
```

**Diagnóstico:** Modelo experimental/incompleto no repositório

## 6. Solução Final: Modelo Funcional

### Modelo Escolhido: `InterSync/Mistral-7B-Instruct-v0.2-Function-Calling`
**Justificativas:**
- Baseado no Mistral-7B-Instruct-v0.2 (modelo robusto)
- Fine-tuned especificamente para function calling
- Arquivos completos e funcionais
- Compatible com quantização 4-bit

### Configuração Técnica
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
```

## 7. Resultados da Execução Final

### Performance Técnica
- **VRAM utilizada:** 3.8GB (bem abaixo do limite de 15GB)
- **Tempo de carregamento:** ~2 minutos (download de 15 arquivos)
- **Formato de arquivos:** safetensors (formato seguro)

### Qualidade das Respostas

#### Reasoning Complexo
**Prompt:** "Resolver problema de velocidade de trem"
**Resultado:** ✅ Solução estruturada, passo-a-passo correto, cálculos precisos

#### Function Calling
**Prompt:** "Criar função para área de formas geométricas"
**Resultado:** ✅ Código Python funcional, estrutura clara

#### Agente Técnico
**Prompt:** "Implementar autenticação JWT com FastAPI"
**Resultado:** ✅ Explicação detalhada, código estruturado, boas práticas

#### Modo Criativo
**Prompt:** "História sobre robô que descobre sentimentos"
**Resultado:** ✅ Narrativa envolvente, desenvolvimento de personagem

## 8. Aprendizados Técnicos

### Sobre Seleção de Modelos
1. **Tamanho ≠ Qualidade:** Modelos menores bem treinados superam modelos grandes genéricos
2. **Especialização importa:** Modelos fine-tuned para tarefas específicas são superiores
3. **Disponibilidade de arquivos:** Verificar integridade do repositório antes de usar

### Sobre Controle de Comportamento
1. **Parâmetros de geração** são mais eficazes que modificação de config
2. **Engenharia de prompt** + parâmetros = controle granular
3. **Quantização 4-bit** permite usar modelos maiores sem perda significativa

### Sobre Desenvolvimento
1. **Gerenciamento de dependências:** protobuf, sentencepiece são críticos para Mistral
2. **Error handling:** Modelos podem ter problemas de integridade
3. **VRAM monitoring:** Quantização reduz uso em ~75%

## 9. Considerações para o TCC

### Pontos Fortes da Abordagem
- Controle granular de comportamento via parâmetros
- Capacidade de criar "personalidades" distintas
- Eficiência de recursos (3.8GB vs 15GB disponíveis)
- Qualidade de resposta adequada para agentes de software

### Limitações Identificadas
- Dependência de modelos externos (Hugging Face)
- Necessidade de hardware específico (GPU)
- Tempo de carregamento inicial significativo
- Complexidade de configuração de dependências

### Próximos Passos Sugeridos
1. **Benchmarking:** Comparar diferentes modelos Mistral
2. **Fine-tuning:** Treinar modelo específico para tarefas do TCC
3. **Integração:** Desenvolver API wrapper para facilitar uso
4. **Otimização:** Explorar modelos menores para deployment

## 10. Conclusões Preliminares

A experimentação demonstrou que é **viável** usar modelos LLM locais para criar agentes de software com comportamentos controlados. O `InterSync/Mistral-7B-Instruct-v0.2-Function-Calling` mostrou-se adequado para:

- ✅ Reasoning estruturado
- ✅ Geração de código funcional  
- ✅ Explanação técnica detalhada
- ✅ Criatividade controlada
- ✅ Eficiência de recursos

**Recomendação:** Prosseguir com este modelo como base para desenvolvimento do sistema de agentes proposto no TCC.